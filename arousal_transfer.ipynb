{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arousal style transfer code demo\n",
    "This notebook provides demonstration on performing arousal style transfer\n",
    "The pre-trained model used here is trained on single bar segments (3-6 seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some dependencies are for our own version of Magenta\n",
    "!pip install torch\n",
    "!pip install tensorflow==1.15.0\n",
    "!pip install sklearn, numpy, matplotlib\n",
    "!pip install pretty_midi\n",
    "!pip install pypianoroll\n",
    "!pip install music21\n",
    "!pip install pygtrie\n",
    "!pip install tensor2tensor\n",
    "!pip install pyfluidsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This repo uses my own version of Magenta. Clone it and keep only the code \n",
    "!git clone https://github.com/gudgud96/magenta.git\n",
    "!mv magenta/magenta magenta_core\n",
    "!rm -r magenta\n",
    "!mv magenta_core magenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded pre-processed VGMIDI dataset melody chunks\n",
    "!mkdir data\n",
    "!mkdir data/filtered_songs_disambiguate\n",
    "!wget https://github.com/gudgud96/music-fader-nets/releases/download/1.0.0/arousal_lst.npy\n",
    "!wget https://github.com/gudgud96/music-fader-nets/releases/download/1.0.0/chroma_lst.npy\n",
    "!wget https://github.com/gudgud96/music-fader-nets/releases/download/1.0.0/note_lst.npy\n",
    "!wget https://github.com/gudgud96/music-fader-nets/releases/download/1.0.0/rhythm_lst.npy\n",
    "!wget https://github.com/gudgud96/music-fader-nets/releases/download/1.0.0/song_tokens.npy\n",
    "!wget https://github.com/gudgud96/music-fader-nets/releases/download/1.0.0/valence_lst.npy\n",
    "!mv *.npy data/filtered_songs_disambiguate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from gmm_model import *\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ptb_v2 import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "from polyphonic_event_based_v2 import *\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from polyphonic_event_based_v2 import parse_pretty_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "with open('gmm_model_config.json') as f:\n",
    "    args = json.load(f)\n",
    "if not os.path.isdir('log'):\n",
    "    os.mkdir('log')\n",
    "if not os.path.isdir('params'):\n",
    "    os.mkdir('params')\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = str(datetime.now())\n",
    "save_path_timing = 'params/{}.pt'.format(args['name'] + \"_\" + timestamp)\n",
    "\n",
    "# Model dimensions\n",
    "EVENT_DIMS = 342\n",
    "RHYTHM_DIMS = 3\n",
    "NOTE_DIMS = 16\n",
    "CHROMA_DIMS = 24\n",
    "\n",
    "# Load model\n",
    "model = MusicAttrRegGMVAE(roll_dims=EVENT_DIMS, rhythm_dims=RHYTHM_DIMS, note_dims=NOTE_DIMS, \n",
    "                        chroma_dims=CHROMA_DIMS,\n",
    "                        hidden_dims=args['hidden_dim'], z_dims=args['z_dim'], \n",
    "                        n_step=args['time_step'],\n",
    "                        n_component=2)  \n",
    "model.load_state_dict(torch.load(\"params/music_attr_vae_reg_gmm.pt\"))\n",
    "print(\"Loading params/music_attr_vae_reg_gmm.pt...\")\n",
    "    \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('CPU mode')\n",
    "\n",
    "step, pre_epoch = 0, 0\n",
    "batch_size = args[\"batch_size\"]\n",
    "is_shuffle = False\n",
    "\n",
    "# ================ In this example, we will load only the examples from VGMIDI dataset ========== #\n",
    "# print(\"Loading Yamaha...\")\n",
    "# data_lst, rhythm_lst, note_density_lst, chroma_lst = get_classic_piano()\n",
    "# tlen, vlen = int(0.8 * len(data_lst)), int(0.9 * len(data_lst))\n",
    "# train_ds_dist = YamahaDataset(data_lst, rhythm_lst, note_density_lst, \n",
    "#                                 chroma_lst, mode=\"train\")\n",
    "# train_dl_dist = DataLoader(train_ds_dist, batch_size=batch_size, shuffle=is_shuffle, num_workers=0)\n",
    "# val_ds_dist = YamahaDataset(data_lst, rhythm_lst, note_density_lst, \n",
    "#                                 chroma_lst, mode=\"val\")\n",
    "# val_dl_dist = DataLoader(val_ds_dist, batch_size=batch_size, shuffle=is_shuffle, num_workers=0)\n",
    "# test_ds_dist = YamahaDataset(data_lst, rhythm_lst, note_density_lst, \n",
    "#                                 chroma_lst, mode=\"test\")\n",
    "# test_dl_dist = DataLoader(test_ds_dist, batch_size=batch_size, shuffle=is_shuffle, num_workers=0)\n",
    "# dl = train_dl_dist\n",
    "# print(len(train_ds_dist), len(val_ds_dist), len(test_ds_dist))\n",
    "\n",
    "# vgmidi dataloaders\n",
    "print(\"Loading VGMIDI...\")\n",
    "data_lst, rhythm_lst, note_density_lst, chroma_lst, arousal_lst, valence_lst = get_vgmidi()\n",
    "vgm_train_ds_dist = VGMIDIDataset(data_lst, rhythm_lst, note_density_lst, \n",
    "                                chroma_lst, arousal_lst, valence_lst, mode=\"train\")\n",
    "vgm_train_dl_dist = DataLoader(vgm_train_ds_dist, batch_size=32, shuffle=is_shuffle, num_workers=0)\n",
    "vgm_val_ds_dist = VGMIDIDataset(data_lst, rhythm_lst, note_density_lst, \n",
    "                                chroma_lst, arousal_lst, valence_lst, mode=\"val\")\n",
    "vgm_val_dl_dist = DataLoader(vgm_val_ds_dist, batch_size=32, shuffle=is_shuffle, num_workers=0)\n",
    "vgm_test_ds_dist = VGMIDIDataset(data_lst, rhythm_lst, note_density_lst, \n",
    "                                chroma_lst, arousal_lst, valence_lst, mode=\"test\")\n",
    "vgm_test_dl_dist = DataLoader(vgm_test_ds_dist, batch_size=32, shuffle=is_shuffle, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(input, dims):\n",
    "    if len(input.shape) > 1:\n",
    "        input_oh = torch.zeros((input.shape[0], input.shape[1], dims)).cuda()\n",
    "        input_oh = input_oh.scatter_(-1, input.unsqueeze(-1), 1.)\n",
    "    else:\n",
    "        input_oh = torch.zeros((input.shape[0], dims)).cuda()\n",
    "        input_oh = input_oh.scatter_(-1, input.unsqueeze(-1), 1.)\n",
    "    return input_oh\n",
    "\n",
    "def clean_output(out):\n",
    "    recon = np.trim_zeros(torch.argmax(out, dim=-1).cpu().detach().numpy().squeeze())\n",
    "    if 1 in recon:\n",
    "        last_idx = np.argwhere(recon == 1)[0][0]\n",
    "        recon[recon == 1] = 0\n",
    "        recon = recon[:last_idx]\n",
    "    return recon\n",
    "\n",
    "def repar(mu, stddev, sigma=1):\n",
    "    eps = Normal(0, sigma).sample(sample_shape=stddev.size()).cuda()\n",
    "    z = mu + stddev * eps  # reparameterization trick\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain \"shifting vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting vectors are obtained by getting the pre-trained mean vector from low arousal and high arousal cluster\n",
    "# low --> high: mean_high - mean_low, vice versa\n",
    "# we need shifting vectors for both rhythm and note space\n",
    "\n",
    "mu_r_lst = []\n",
    "var_r_lst = []\n",
    "mu_n_lst = []\n",
    "var_n_lst = []\n",
    "for k_i in torch.arange(0, 2):\n",
    "    mu_k = model.mu_r_lookup(k_i.cuda())\n",
    "    mu_r_lst.append(mu_k.cpu().detach())\n",
    "    \n",
    "    var_k = model.logvar_r_lookup(k_i.cuda()).exp_()\n",
    "    var_r_lst.append(var_k.cpu().detach())\n",
    "    \n",
    "    mu_k = model.mu_n_lookup(k_i.cuda())\n",
    "    mu_n_lst.append(mu_k.cpu().detach())\n",
    "    \n",
    "    var_k = model.logvar_n_lookup(k_i.cuda()).exp_()\n",
    "    var_n_lst.append(var_k.cpu().detach())\n",
    "\n",
    "r_low_to_high = mu_r_lst[1] - mu_r_lst[0]\n",
    "r_high_to_low = mu_r_lst[0] - mu_r_lst[1]\n",
    "n_low_to_high = mu_n_lst[1] - mu_n_lst[0]\n",
    "n_high_to_low = mu_n_lst[0] - mu_n_lst[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load base melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use a melody segment from the VGMIDI test set\n",
    "# Choose any number between 0 - 51 for `idx` variable\n",
    "# Alternatively, you can also encode your desired melody segment using `magenta_encode_midi` in `ptb_v2.py`\n",
    "# and use the token sequence as `d` here\n",
    "idx = 5\n",
    "d, r, n, a, v, c, r_density, n_density = vgm_test_ds_dist[idx]\n",
    "c = torch.Tensor(c).cuda().unsqueeze(0)\n",
    "\n",
    "# Print the encoded event tokens\n",
    "eos_index = np.where(d==1)[0][0]\n",
    "print(\"Input tokens:\", d.int().numpy()[:eos_index])\n",
    "\n",
    "# Decode it into MIDI and listen the segment\n",
    "# Note: you need to pre-install fluidsynth (using apt-get on linux) and pyfluidsynth (using pip)\n",
    "d_oh = convert_to_one_hot(torch.Tensor(d).cuda().long(), EVENT_DIMS)\n",
    "pm = magenta_decode_midi(d.int().numpy()[:eos_index])\n",
    "a_1 = pm.fluidsynth()\n",
    "Audio(a_1, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low arousal --> high arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dis_r, dis_n = model.encode(d_oh.unsqueeze(0))\n",
    "z_r = dis_r.rsample()\n",
    "z_n = dis_n.rsample()\n",
    "\n",
    "# lmbda is a parameter for you to control `how much` is the extent of transfer\n",
    "# if you think the transferred arousal of output is not high enough, increase lmbda (and vice versa)\n",
    "lmbda = 1\n",
    "z_r_new = z_r + lmbda*torch.Tensor(r_low_to_high).cuda()\n",
    "z_n_new = z_n + lmbda*torch.Tensor(n_low_to_high).cuda()\n",
    "\n",
    "z = torch.cat([z_r_new, z_n_new, c], dim=1)        \n",
    "out = model.global_decoder(z, steps=300)\n",
    "print(\"Tokens:\", clean_output(out))\n",
    "\n",
    "# Listen to the transferred output\n",
    "pm = magenta_decode_midi(clean_output(out))\n",
    "a_1 = pm.fluidsynth()\n",
    "Audio(a_1, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on high arousal -> low arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dis_r, dis_n = model.encode(d_oh.unsqueeze(0))\n",
    "z_r = dis_r.rsample()\n",
    "z_n = dis_n.rsample()\n",
    "\n",
    "# lmbda is a parameter for you to control `how much` is the extent of transfer\n",
    "# if you think the transferred arousal of output is not low enough, increase lmbda (and vice versa)\n",
    "lmbda = 1\n",
    "z_r_new = z_r + lmbda*torch.Tensor(r_high_to_low).cuda()\n",
    "z_n_new = z_n + lmbda*torch.Tensor(n_high_to_low).cuda()\n",
    "\n",
    "z = torch.cat([z_r_new, z_n_new, c], dim=1)        \n",
    "out = model.global_decoder(z, steps=300)\n",
    "print(\"Tokens:\", clean_output(out))\n",
    "\n",
    "# Listen to the transferred output\n",
    "pm = magenta_decode_midi(clean_output(out))\n",
    "a_1 = pm.fluidsynth()\n",
    "Audio(a_1, rate=44100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
